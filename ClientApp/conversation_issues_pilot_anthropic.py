#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Copyright (c) 2026 Cisco and/or its affiliates.
This software is licensed to you under the terms of the Cisco Sample
Code License, Version 1.1 (the "License"). You may obtain a copy of the
License at
               https://developer.cisco.com/docs/licenses
All use of the material herein must be in accordance with the terms of
the License. All rights not expressly granted by the License are
reserved. Unless required by applicable law or agreed to separately in
writing, software distributed under the License is distributed on an "AS
IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
or implied.
"""

__author__ = "Gabriel Zapodeanu TME, ENB"
__email__ = "gzapodea@cisco.com"
__version__ = "0.1.0"
__copyright__ = "Copyright (c) 2026 Cisco and/or its affiliates."
__license__ = "Cisco Sample Code License, Version 1.1"

import os
import chromadb

from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_anthropic import ChatAnthropic
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

load_dotenv('environment.env')

# database server details
DB_SERVER = os.getenv('DB_SERVER')
DB_PORT = int(os.getenv('DB_PORT'))
DB_COLLECTION = os.getenv('DB_COLLECTION')

os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# Embeddings model
MODEL_NAME = os.getenv('MODEL_NAME')

# Claude config
CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY')
LLM_MODEL = os.getenv('CLAUDE_MODEL')


def main():
    """
    A query app for network troubleshooting, powered by LangChain, that retrieves
    similarity matches from Chroma and generates responses using Claude Sonnet 4.
    """

    # Chroma DB server details and connection
    chroma_db_server = chromadb.HttpClient(host=DB_SERVER, port=DB_PORT)

    # Define the embeddings model
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)

    # Chroma DB connection to server and collection
    chroma_db = Chroma(
        client=chroma_db_server,
        collection_name=DB_COLLECTION,
        embedding_function=embeddings
    )

    # Define retriever from Chroma DB and number of proximity matches
    retriever = chroma_db.as_retriever(search_kwargs={"k": 10})

    # Define the LLM used - Claude Sonnet 4
    llm = ChatAnthropic(
        model=LLM_MODEL,
        anthropic_api_key=CLAUDE_API_KEY,
        temperature=1,
        max_tokens=1024

    )

    # Create the history prompt template
    genaiops_prompt = (
        "You are an assistant for network troubleshooting tasks. "
        "Use the following pieces of retrieved context to answer the question. "
        "Consider the conversation history when answering follow-up questions. "
        "If the user refers to previous topics (like 'that', 'it', 'those steps'), "
        "use the chat history to understand what they're referring to. "
        "If you don't know the answer, say that you don't know. "
        "The user is networking knowledgeable."
        "\n\n"
        "Retrieved Context:\n{context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", genaiops_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    # Create the question answer chain
    question_answer_chain = create_stuff_documents_chain(llm, prompt)

    # Create the conversational retrieval chain
    conversational_chain = create_retrieval_chain(retriever, question_answer_chain)

    print('\nHi, I am your IssuesPilot! Enter your query or press Enter to end.\n')

    # Initialize chat history
    chat_history = []

    while True:
        # Prompt user for input
        query = input("Your input: ").strip()

        if query == '':
            print('\nIssuesPilot. Goodbye!\n')
            break

        # Generate response using conversational chain
        response = conversational_chain.invoke({
            "input": query,
            "chat_history": chat_history
        })

        print('IssuesPilot: ' + response['answer'] + '\n')

        # Update chat history
        chat_history.extend([
            HumanMessage(content=query),
            AIMessage(content=response['answer'])
        ])

        # Limit message history to prevent context window overflow
        if len(chat_history) > 10:
            chat_history = chat_history[-10:]

    return


if __name__ == "__main__":
    main()